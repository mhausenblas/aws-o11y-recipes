{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 We are covering recipes for observability (o11y) solutions at AWS on this site. This includes managed services such as Amazon Managed Service for Prometheus (AMP) as well as libraries & agents, for example as provided by OpenTelemetry or Fluent Bit . We want to address the needs of both developers and infrastructure folks. The way we think about the o11y space is as follows: we decompose it into six dimensions you can then combine to arrive at a specifc solution: dimension examples destinations AMP \u00b7 AMG \u00b7 CW \u00b7 X-Ray \u00b7 Jaeger AES \u00b7 S3 \u00b7 Kafka agents ADOT \u00b7 Fluent Bit \u00b7 CW agent \u00b7 X-Ray agent languages Java \u00b7 Python \u00b7 .NET \u00b7 JavaScript \u00b7 Go \u00b7 Rust infra & databases VPC flow logs \u00b7 EKS CP \u00b7 exporters S3 mon \u00b7 SQS tracing \u00b7 RDS mon \u00b7 DynamoDB compute unit Batch \u00b7 ECS \u00b7 EKS \u00b7 AEB \u00b7 Lambda compute engine Fargate \u00b7 EC2 \u00b7 Lightsail For example, you might be looking for a solution to: Examplary solution specification I need a logging solution for a Python app I'm running on EKS on Fargate with the goal to store the logs in an S3 bucket for further consumption Destination : S3 bucket for further consumption Agent : FluentBit Language : Python Infra & DB : N/A Compute unit : Kubernetes (EKS) Compute engine : EC2 Not every dimension needs to be specified and sometimes it's hard to decide where to start. Try different paths and compare the pros and cons of certain recipes. To simplify navigation, we're grouping the six dimension into the following catagories: By Compute : covering compute engines and units By Infra & Data : covering infrastructure and databases By Language : covering languages By Destination : covering telemetry and analytics Tasks : covering anomaly detection, alerting, troubleshooting, and more Learn more about dimensions \u2026 How to use \u00b6 You can either use the top navigation menu to browse to a specific index page, starting with a rough selection. For example, By Compute -> EKS -> Fargate -> Logs . Alternatively, you can search the site pressing / or the s key: License All recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution. How to contribute \u00b6 Start a discussion on what you plan to do and we take it from there. Learn more \u00b6 The recipes on this site are a good practices collection. In addition, there are a number of places where you can learn more about the status of open source projects we use as well as about the managed services from the recipes, so check out: observability @ aws , a playlist of AWS folks talking about their projects and services. AWS observability workshops , to try out the offerings in a structured manner. The AWS monitoring and observability homepage with pointers to case studies and partners.","title":"Welcome!"},{"location":"#welcome","text":"We are covering recipes for observability (o11y) solutions at AWS on this site. This includes managed services such as Amazon Managed Service for Prometheus (AMP) as well as libraries & agents, for example as provided by OpenTelemetry or Fluent Bit . We want to address the needs of both developers and infrastructure folks. The way we think about the o11y space is as follows: we decompose it into six dimensions you can then combine to arrive at a specifc solution: dimension examples destinations AMP \u00b7 AMG \u00b7 CW \u00b7 X-Ray \u00b7 Jaeger AES \u00b7 S3 \u00b7 Kafka agents ADOT \u00b7 Fluent Bit \u00b7 CW agent \u00b7 X-Ray agent languages Java \u00b7 Python \u00b7 .NET \u00b7 JavaScript \u00b7 Go \u00b7 Rust infra & databases VPC flow logs \u00b7 EKS CP \u00b7 exporters S3 mon \u00b7 SQS tracing \u00b7 RDS mon \u00b7 DynamoDB compute unit Batch \u00b7 ECS \u00b7 EKS \u00b7 AEB \u00b7 Lambda compute engine Fargate \u00b7 EC2 \u00b7 Lightsail For example, you might be looking for a solution to: Examplary solution specification I need a logging solution for a Python app I'm running on EKS on Fargate with the goal to store the logs in an S3 bucket for further consumption Destination : S3 bucket for further consumption Agent : FluentBit Language : Python Infra & DB : N/A Compute unit : Kubernetes (EKS) Compute engine : EC2 Not every dimension needs to be specified and sometimes it's hard to decide where to start. Try different paths and compare the pros and cons of certain recipes. To simplify navigation, we're grouping the six dimension into the following catagories: By Compute : covering compute engines and units By Infra & Data : covering infrastructure and databases By Language : covering languages By Destination : covering telemetry and analytics Tasks : covering anomaly detection, alerting, troubleshooting, and more Learn more about dimensions \u2026","title":"Welcome!"},{"location":"#how-to-use","text":"You can either use the top navigation menu to browse to a specific index page, starting with a rough selection. For example, By Compute -> EKS -> Fargate -> Logs . Alternatively, you can search the site pressing / or the s key: License All recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution.","title":"How to use"},{"location":"#how-to-contribute","text":"Start a discussion on what you plan to do and we take it from there.","title":"How to contribute"},{"location":"#learn-more","text":"The recipes on this site are a good practices collection. In addition, there are a number of places where you can learn more about the status of open source projects we use as well as about the managed services from the recipes, so check out: observability @ aws , a playlist of AWS folks talking about their projects and services. AWS observability workshops , to try out the offerings in a structured manner. The AWS monitoring and observability homepage with pointers to case studies and partners.","title":"Learn more"},{"location":"aes/","text":"Amazon Elasticsearch Service(AES) \u00b6 Getting started with AES Log Analytics with AES Getting started with Open Distro for Elasticsearch Know your data with Machine Learning Send CloudTrail Logs to AES Searching DynamoDB Data with AES Getting Started with Trace Analytics in AES","title":"AES"},{"location":"aes/#amazon-elasticsearch-serviceaes","text":"Getting started with AES Log Analytics with AES Getting started with Open Distro for Elasticsearch Know your data with Machine Learning Send CloudTrail Logs to AES Searching DynamoDB Data with AES Getting Started with Trace Analytics in AES","title":"Amazon Elasticsearch Service(AES)"},{"location":"alerting/","text":"Alerting \u00b6 This section has a selection of recipes for various alerting systems and scenarios. Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS","title":"Alerting"},{"location":"alerting/#alerting","text":"This section has a selection of recipes for various alerting systems and scenarios. Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS","title":"Alerting"},{"location":"amg/","text":"Amazon Managed Service for Grafana (AMG) \u00b6 Getting Started with AMG Monitoring hybrid environments using Amazon Managed Service for Grafana Integrating identity providers (OneLogin, Ping Identity, Okta, and Azure AD) to SSO into AMG Workshop for Getting Started with AMG","title":"AMG"},{"location":"amg/#amazon-managed-service-for-grafana-amg","text":"Getting Started with AMG Monitoring hybrid environments using Amazon Managed Service for Grafana Integrating identity providers (OneLogin, Ping Identity, Okta, and Azure AD) to SSO into AMG Workshop for Getting Started with AMG","title":"Amazon Managed Service for Grafana (AMG)"},{"location":"amp/","text":"Amazon Managed Service for Prometheus (AMP) \u00b6 Getting Started with AMP Using ADOT in EKS on EC2 to ingest to AMP and visualize in AMG Setting up cross-account ingestion into AMP Metrics collection from Amazon ECS using AMP Configuring Grafana Cloud Agent for AMP Set up cross-region metrics collection for AMP workspaces Workshop for Getting Started with AMP","title":"AMP"},{"location":"amp/#amazon-managed-service-for-prometheus-amp","text":"Getting Started with AMP Using ADOT in EKS on EC2 to ingest to AMP and visualize in AMG Setting up cross-account ingestion into AMP Metrics collection from Amazon ECS using AMP Configuring Grafana Cloud Agent for AMP Set up cross-region metrics collection for AMP workspaces Workshop for Getting Started with AMP","title":"Amazon Managed Service for Prometheus (AMP)"},{"location":"anomaly-detection/","text":"Anomaly Detection \u00b6 This section contains recipes for anomaly detection. Enabling Anomaly Detection for a CloudWatch Metric","title":"Anomaly Detection"},{"location":"anomaly-detection/#anomaly-detection","text":"This section contains recipes for anomaly detection. Enabling Anomaly Detection for a CloudWatch Metric","title":"Anomaly Detection"},{"location":"cw/","text":"Amazon CloudWatch (CW) \u00b6 Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS Monitoring ECS containerized Applications and Microservices using CW Container Insights Monitoring EKS containerized Applications and Microservices using CW Container Insights Create Canaries via CW Synthetics Cloudwatch Logs Insights for Quering Logs Lamda Insights Anamoly Detection vi CloudWatch Metrics Alarms via CloudWatch","title":"CW"},{"location":"cw/#amazon-cloudwatch-cw","text":"Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS Monitoring ECS containerized Applications and Microservices using CW Container Insights Monitoring EKS containerized Applications and Microservices using CW Container Insights Create Canaries via CW Synthetics Cloudwatch Logs Insights for Quering Logs Lamda Insights Anamoly Detection vi CloudWatch Metrics Alarms via CloudWatch","title":"Amazon CloudWatch (CW)"},{"location":"dimensions/","text":"Dimensions \u00b6 In the context of this site we consider the o11y space along six dimensions. Looking at each dimension independently is beneficial from an synthetic point-of-view, that is, when you're trying to build out a concrete o11y solution for a given workload, spanning developer-related aspects such as the programming language used as well as operational topics, for example the runtime environment like containers or Lambda functions. What is a signal? When we say signal here we mean any kinds of o11y data and metadata points, including log entries, metrics, and traces. Unless we want to or have to be more specific, we use \"signal\" and it should be clear from the context what restrictions may apply. Let's now have a look at each of the six dimensions one by one: Destinations \u00b6 In this dimension we consider all kinds of signal destinations including long term storage and graphical interfaces that let you consume signals. As a developer, you want access to an UI or an API that allows you to discover, look up, and correlate signals to troubleshoot your service. In an infrastructure or platform role you want access to an UI or an API that allows you to manage, discover, look up, and correlate signals to understand the state of the infrastructure. Ultimately, this is the most interesting dimension from a human point of view. However, in order to be able to reap the benefits we first have to invest a bit of work: we need to instrument our software and external dependencies and ingest the signals into the destinations. So, how do the signals arrive in the destinations? Glad you asked, it's \u2026 Agents \u00b6 How the signals are collected and routed to analytics. The signals can come from two sources: either your application source code (see also the language section) or from things your application depends on, such as state managed in datastores as well as infrastructure like VPCs (see also the infra & data section). Agents are part of the telemetry that you would use to collect and ingest signals. The other part are the instrumented applications and infra pieces like databases. Languages \u00b6 This dimension is concerned with the programming language you use for writing your service or application. Here, we're dealing with SDKs and libraries, such as the X-Ray SDKs or what OpenTelemetry provides in the context of instrumentation . You want to make sure that an o11y solution supports your programming language of choice for a given signal type such as logs or metrics. Infrastructure & databases \u00b6 With this dimension we mean any sort of application-external dependencies, be it infrastructure like the VPC the service is running in or a datastore like RDS or DynamoDB or a queue like SQS. Commonalities One thing all the sources in this dimension have in common is that they are located outside of your application (as well as the compute environment your app runs in) and with that you have to treat them as an opaque box. This dimension includes but is not limited to: AWS infrastructure, for example VPC flow logs . Secondary APIs such as Kubernetes control plane logs . Signals from datastores, such as or S3 , RDS or SQS . Compute unit \u00b6 The way your package, schedule, and run your code. For example, in Lambda that's a function and in ECS and EKS that unit is a container running in a tasks (ECS) or pods (EKS), respectively. Containerized environments like Kubernetes often allow for two options concerning telemetry deployments: as side cars or as per-node (instance) daemon processes. Compute engine \u00b6 This dimension refers to the base runtime environment, which may (in case of an EC2 instance, for example) or may not (serverless offerings such as Fargate or Lambda) be your responsibility to provision and patch. Depending on the compute engine you use, the telemetry part might already be part of the offering, for example, EKS on Fargate has log routing via Fluent Bit integrated.","title":"Dimensions"},{"location":"dimensions/#dimensions","text":"In the context of this site we consider the o11y space along six dimensions. Looking at each dimension independently is beneficial from an synthetic point-of-view, that is, when you're trying to build out a concrete o11y solution for a given workload, spanning developer-related aspects such as the programming language used as well as operational topics, for example the runtime environment like containers or Lambda functions. What is a signal? When we say signal here we mean any kinds of o11y data and metadata points, including log entries, metrics, and traces. Unless we want to or have to be more specific, we use \"signal\" and it should be clear from the context what restrictions may apply. Let's now have a look at each of the six dimensions one by one:","title":"Dimensions"},{"location":"dimensions/#destinations","text":"In this dimension we consider all kinds of signal destinations including long term storage and graphical interfaces that let you consume signals. As a developer, you want access to an UI or an API that allows you to discover, look up, and correlate signals to troubleshoot your service. In an infrastructure or platform role you want access to an UI or an API that allows you to manage, discover, look up, and correlate signals to understand the state of the infrastructure. Ultimately, this is the most interesting dimension from a human point of view. However, in order to be able to reap the benefits we first have to invest a bit of work: we need to instrument our software and external dependencies and ingest the signals into the destinations. So, how do the signals arrive in the destinations? Glad you asked, it's \u2026","title":"Destinations"},{"location":"dimensions/#agents","text":"How the signals are collected and routed to analytics. The signals can come from two sources: either your application source code (see also the language section) or from things your application depends on, such as state managed in datastores as well as infrastructure like VPCs (see also the infra & data section). Agents are part of the telemetry that you would use to collect and ingest signals. The other part are the instrumented applications and infra pieces like databases.","title":"Agents"},{"location":"dimensions/#languages","text":"This dimension is concerned with the programming language you use for writing your service or application. Here, we're dealing with SDKs and libraries, such as the X-Ray SDKs or what OpenTelemetry provides in the context of instrumentation . You want to make sure that an o11y solution supports your programming language of choice for a given signal type such as logs or metrics.","title":"Languages"},{"location":"dimensions/#infrastructure-databases","text":"With this dimension we mean any sort of application-external dependencies, be it infrastructure like the VPC the service is running in or a datastore like RDS or DynamoDB or a queue like SQS. Commonalities One thing all the sources in this dimension have in common is that they are located outside of your application (as well as the compute environment your app runs in) and with that you have to treat them as an opaque box. This dimension includes but is not limited to: AWS infrastructure, for example VPC flow logs . Secondary APIs such as Kubernetes control plane logs . Signals from datastores, such as or S3 , RDS or SQS .","title":"Infrastructure &amp; databases"},{"location":"dimensions/#compute-unit","text":"The way your package, schedule, and run your code. For example, in Lambda that's a function and in ECS and EKS that unit is a container running in a tasks (ECS) or pods (EKS), respectively. Containerized environments like Kubernetes often allow for two options concerning telemetry deployments: as side cars or as per-node (instance) daemon processes.","title":"Compute unit"},{"location":"dimensions/#compute-engine","text":"This dimension refers to the base runtime environment, which may (in case of an EC2 instance, for example) or may not (serverless offerings such as Fargate or Lambda) be your responsibility to provision and patch. Depending on the compute engine you use, the telemetry part might already be part of the offering, for example, EKS on Fargate has log routing via Fluent Bit integrated.","title":"Compute engine"},{"location":"dynamodb/","text":"DynamoDB \u00b6 Searching DynamoDB data with Amazon Elasticsearch Service DynamoDB Contributor Insights","title":"DynamoDB"},{"location":"dynamodb/#dynamodb","text":"Searching DynamoDB data with Amazon Elasticsearch Service DynamoDB Contributor Insights","title":"DynamoDB"},{"location":"ecs/","text":"ECS \u00b6 ECS on EC2 \u00b6 Logs \u00b6 Under the hood: FireLens for Amazon ECS Tasks Metrics \u00b6 Using AWS Distro for OpenTelemetry collector for cross-account metrics collection on Amazon ECS Metrics collection from ECS using Amazon Managed Service for Prometheus ECS on Fargate \u00b6 Logs \u00b6 Sample logging architectures for FireLens on Amazon ECS and AWS Fargate using Fluent Bit","title":"ECS"},{"location":"ecs/#ecs","text":"","title":"ECS"},{"location":"ecs/#ecs-on-ec2","text":"","title":"ECS on EC2"},{"location":"ecs/#logs","text":"Under the hood: FireLens for Amazon ECS Tasks","title":"Logs"},{"location":"ecs/#metrics","text":"Using AWS Distro for OpenTelemetry collector for cross-account metrics collection on Amazon ECS Metrics collection from ECS using Amazon Managed Service for Prometheus","title":"Metrics"},{"location":"ecs/#ecs-on-fargate","text":"","title":"ECS on Fargate"},{"location":"ecs/#logs_1","text":"Sample logging architectures for FireLens on Amazon ECS and AWS Fargate using Fluent Bit","title":"Logs"},{"location":"eks/","text":"EKS \u00b6 EKS on EC2 \u00b6 Logs \u00b6 Fluent Bit Integration in CloudWatch Container Insights for EKS Logging with EFK Stack Sample logging architectures for Fluent Bit and FluentD on EKS Metrics \u00b6 Getting Started with Amazon Managed Service for Prometheus Using ADOT in EKS on EC2 to ingest to AMP and visualize in AMG Configuring Grafana Cloud Agent for Amazon Managed Service for Prometheus Monitoring cluster using Prometheus and Grafana Monitoring with Managed Prometheus and Managed Grafana CloudWatch Container Insights Set up cross-region metrics collection for AMP workspaces Monitoring App Mesh environment on EKS using Amazon Managed Service for Prometheus Traces \u00b6 Migrating X-Ray tracing to AWS Distro for OpenTelemetry Tracing with X-Ray EKS on Fargate \u00b6 Logs \u00b6 Fluent Bit for Amazon EKS on AWS Fargate is here Sample logging architectures for Fluent Bit and FluentD on EKS Metrics \u00b6 CloudWatch Container Insights Set up cross-region metrics collection for AMP workspaces Traces \u00b6 Tracing with X-Ray","title":"EKS"},{"location":"eks/#eks","text":"","title":"EKS"},{"location":"eks/#eks-on-ec2","text":"","title":"EKS on EC2"},{"location":"eks/#logs","text":"Fluent Bit Integration in CloudWatch Container Insights for EKS Logging with EFK Stack Sample logging architectures for Fluent Bit and FluentD on EKS","title":"Logs"},{"location":"eks/#metrics","text":"Getting Started with Amazon Managed Service for Prometheus Using ADOT in EKS on EC2 to ingest to AMP and visualize in AMG Configuring Grafana Cloud Agent for Amazon Managed Service for Prometheus Monitoring cluster using Prometheus and Grafana Monitoring with Managed Prometheus and Managed Grafana CloudWatch Container Insights Set up cross-region metrics collection for AMP workspaces Monitoring App Mesh environment on EKS using Amazon Managed Service for Prometheus","title":"Metrics"},{"location":"eks/#traces","text":"Migrating X-Ray tracing to AWS Distro for OpenTelemetry Tracing with X-Ray","title":"Traces"},{"location":"eks/#eks-on-fargate","text":"","title":"EKS on Fargate"},{"location":"eks/#logs_1","text":"Fluent Bit for Amazon EKS on AWS Fargate is here Sample logging architectures for Fluent Bit and FluentD on EKS","title":"Logs"},{"location":"eks/#metrics_1","text":"CloudWatch Container Insights Set up cross-region metrics collection for AMP workspaces","title":"Metrics"},{"location":"eks/#traces_1","text":"Tracing with X-Ray","title":"Traces"},{"location":"infra/","text":"Infrastructure & Databases \u00b6 VPC Flow logs analysis using Amazon Elasticsearch Service","title":"Infra"},{"location":"infra/#infrastructure-databases","text":"VPC Flow logs analysis using Amazon Elasticsearch Service","title":"Infrastructure &amp; Databases"},{"location":"java/","text":"Java \u00b6 StatsD and Java Support in AWS Distro for OpenTelemetry","title":"Java"},{"location":"java/#java","text":"StatsD and Java Support in AWS Distro for OpenTelemetry","title":"Java"},{"location":"lambda/","text":"Lambda \u00b6 Logs \u00b6 Deploy and Monitor a Serverless Application Metrics \u00b6 Introducing CloudWatch Lambda Insights","title":"Lambda"},{"location":"lambda/#lambda","text":"","title":"Lambda"},{"location":"lambda/#logs","text":"Deploy and Monitor a Serverless Application","title":"Logs"},{"location":"lambda/#metrics","text":"Introducing CloudWatch Lambda Insights","title":"Metrics"},{"location":"msk/","text":"Managed Streaming for Apache Kafka (MSK) \u00b6 Amazon Managed Streaming for Apache Kafka: Open Monitoring with Prometheus","title":"MSK"},{"location":"msk/#managed-streaming-for-apache-kafka-msk","text":"Amazon Managed Streaming for Apache Kafka: Open Monitoring with Prometheus","title":"Managed Streaming for Apache Kafka (MSK)"},{"location":"nodejs/","text":"Node.js \u00b6 NodeJS library to generate embedded CloudWatch metrics","title":"Node.js"},{"location":"nodejs/#nodejs","text":"NodeJS library to generate embedded CloudWatch metrics","title":"Node.js"},{"location":"rds/","text":"RDS \u00b6 Amazon CloudWatch Logs for Amazon RDS Performance Insights metrics published to Amazon CloudWatch","title":"RDS"},{"location":"rds/#rds","text":"Amazon CloudWatch Logs for Amazon RDS Performance Insights metrics published to Amazon CloudWatch","title":"RDS"},{"location":"telemetry/","text":"Telemetry \u00b6 Telemetry is all about how the signals are collected from various sources, including your own app and infrastructure and routed to destinations where they are consumed: Let's further dive into the concepts introduced in above figure. Sources \u00b6 We consider sources as something where signals come from. There are two types of sources: Things under your control, that is, the application source code, via instrumentation. Everything else you may use, such as managed services, not under your (direct) control. These types of sources are typically provided by AWS, exposing signals via an API. Agents \u00b6 In order to transpor signals from the sources to the destinations, you need some sort of intermediary we call agent. These agents receive or pull signals from the sources and, typically via configuration, determine where signals shoud go, optionally supporting filtering and aggregation. Agents? Routing? Shipping? Ingesting? There are many terms out there people use to refer to the process of getting the signals from sources to destinations including routing, shipping, aggregation, ingesting etc. and while they may mean slightly different things, we will use them here interchangeably. Canonically, we will refer to those intermediary transport components as agents. Destinations \u00b6 Where signals end up, for consumption. No matter if you want to store signals for later consumption, if you want to dashboard them, set an alert if a certain condition is true, or correlate signals. All of those components that serve you as the end-user are destinations.","title":"Telemetry"},{"location":"telemetry/#telemetry","text":"Telemetry is all about how the signals are collected from various sources, including your own app and infrastructure and routed to destinations where they are consumed: Let's further dive into the concepts introduced in above figure.","title":"Telemetry"},{"location":"telemetry/#sources","text":"We consider sources as something where signals come from. There are two types of sources: Things under your control, that is, the application source code, via instrumentation. Everything else you may use, such as managed services, not under your (direct) control. These types of sources are typically provided by AWS, exposing signals via an API.","title":"Sources"},{"location":"telemetry/#agents","text":"In order to transpor signals from the sources to the destinations, you need some sort of intermediary we call agent. These agents receive or pull signals from the sources and, typically via configuration, determine where signals shoud go, optionally supporting filtering and aggregation. Agents? Routing? Shipping? Ingesting? There are many terms out there people use to refer to the process of getting the signals from sources to destinations including routing, shipping, aggregation, ingesting etc. and while they may mean slightly different things, we will use them here interchangeably. Canonically, we will refer to those intermediary transport components as agents.","title":"Agents"},{"location":"telemetry/#destinations","text":"Where signals end up, for consumption. No matter if you want to store signals for later consumption, if you want to dashboard them, set an alert if a certain condition is true, or correlate signals. All of those components that serve you as the end-user are destinations.","title":"Destinations"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 We include troubleshooting recipes for various situations and dimensions in this section. Troubleshooting performance bottleneck in DynamoDB","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"We include troubleshooting recipes for various situations and dimensions in this section. Troubleshooting performance bottleneck in DynamoDB","title":"Troubleshooting"},{"location":"workshops/","text":"Workshops \u00b6 This section contains workshops to which you can return for samples and demonstrations around o11y systems and tooling. One Observability Workshop EKS Workshop ECS Workshop","title":"Workshops"},{"location":"workshops/#workshops","text":"This section contains workshops to which you can return for samples and demonstrations around o11y systems and tooling. One Observability Workshop EKS Workshop ECS Workshop","title":"Workshops"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/","text":"Using AWS Distro for OpenTelemetry in EKS to ingest metrics into Amazon Managed Service for Prometheus \u00b6 In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus (AMP) . Then we're using Amazon Managed Service for Grafana (AMG) to visualize the metrics. We will be setting up an Amazon Elastic Kubernetes Service (EKS) cluster and Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete. Infrastructure \u00b6 In the following section we will be setting up the infrastructure for this recipe. Architecture \u00b6 The ADOT-AMP pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and send the scraped metrics to AMP. The ADOT Collector includes two AWS OpenTelemetry Collector components specific to Prometheus \u2014 the Prometheus Receiver and the AWS Prometheus Remote Write Exporter. Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have docker installed into your environment. Setup an EKS cluster \u00b6 Our demo application in this recipe will be running on top of EKS. You can either use an existing EKS cluster or create one using cluster_config.yaml . This template will create a new cluster with two EC2 t2.large nodes. Edit the template file and set your region to one of the available regions for AMP: us-east-1 us-east-2 us-west-2 eu-central-1 eu-west-1 Make sure to overwrite this region in your session, for example in bash: export AWS_DEFAULT_REGION=eu-west-1 Create your cluster using the following command. eksctl create cluster -f cluster-config.yaml Setup an ECR repository \u00b6 In order to deploy our application to EKS we need a container registry. You can use the following command to create a new ECR registry in your account. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region eu-west-1 Setup AMP \u00b6 create a workspace using the AWS CLI aws amp create-workspace --alias prometheus-sample-app Verify the workspace is created using: aws amp list-workspaces Info For more details check out the AMP Getting started guide. Setup ADOT Collector \u00b6 Download the template file prometheus-deamonset.yaml and edit this file with the parameters described in the next steps. In this example, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. Use the following steps to edit the downloaded file for your environment: 1. Replace <REGION> with your current Region. 2. Replace <YOUR_ENDPOINT> with your AMP workspace endpoint URL. Get your AMP endpoint url by executing the following query: aws amp describe-workspace \\ --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` \\ --query 'workspace.prometheusEndpoint' 3. Finally replace your <YOUR_ACCOUNT_ID> with your current account ID. The following command will return the account ID for the current session: aws sts get-caller-identity --query Account --output text After creating deployment file we can now apply this to our cluster by using the following command: kubectl apply -f prometheus-deamonset.yaml Info For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup Setup AMG \u00b6 Setup a new AMG workspace using the Amazon Managed Service for Grafana \u2013 Getting Started guide. Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation. Application \u00b6 In this recipe we will be using a sample application from the AWS Observability repository. This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint. Build \u00b6 Clone the following Git repository git clone git@github.com:aws-observability/aws-otel-community.git Build the container cd ./aws-otel-community/sample-apps/prometheus docker build . -t prometheus-sample-app:latest Note If go mod fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile. Change the following line in the Docker file: RUN GO111MODULE=on go mod download to: RUN GOPROXY=direct GO111MODULE=on go mod download Push \u00b6 Change the region variable to the region you selected in the beginning of this guide: export REGION=\"eu-west-1\" export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` Authenticate to your default registry: aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com\" Push container to the ECR repository docker tag prometheus-sample-app:latest \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\" docker push \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\" Deploy \u00b6 Edit prometheus-sample-app.yaml to contain your ECR image path. Edit the deployment to reflect your image path. (example below) apiVersion: apps/v1 kind: Deployment ... spec: containers: - name: prometheus-sample-app image: \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\" # change to your image command: [\"/bin/main\", \"-listen_address=0.0.0.0:8080\", \"-metric_count=10\"] ports: - name: web containerPort: 8080 ... Deploy the sample app to your cluster: kubectl apply -f prometheus-sample-app.yaml End-to-end \u00b6 Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG. Verify your pipeline is working \u00b6 Enter the following command to and note down the name of the collector pod: kubectl -n adot-col get pods You should be able to see a adot-collector pod in the running state: NAME READY STATUS RESTARTS AGE adot-collector-5f7448f6f6-cj7j8 1/1 Running 0 1h Our example template is already integrated with the logging exporter. Enter the following command: kubectl -n adot-col logs adot-collector From of the scraped metrics from the sample app will look like the following example: Resource labels: -> service.name: STRING(kubernetes-service-endpoints) -> host.name: STRING(192.168.16.238) -> port: STRING(8080) -> scheme: STRING(http) InstrumentationLibraryMetrics #0 Metric #0 Descriptor: -> Name: test_gauge0 -> Description: This is my gauge -> Unit: -> DataType: DoubleGauge DoubleDataPoints #0 StartTime: 0 Timestamp: 1606511460471000000 Value: 0.000000 To test whether AMP received the metrics, use awscurl . This tool enables you to send HTTP requests through the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command, and replace $AMP_ENDPOINT with the endpoint for your AMP workspace. awscurl --service=\"aps\" \\ --region=\"$REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" \\ {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}} Create a Grafana dashboard in AMG \u00b6 You can now create a dashboard in Grafana to visualise the data you collected in the test App. Check out the AMG User Guide: Dashboards , to learn more about Grafana dashboards. When creating a new dashboard, make sure it has a meaningful name. If you are creating a dashboard to play or experiment, then put the word TEST or TMP in the name. Consider including your name or initials in the dashboard name or as a tag so that people know who owns the dashboard. Remove temporary experiment dashboards when you are done with them. If you create many related dashboards, think about how to cross-reference them for easy navigation. Refer to Best practices for managing dashboards for more information. Avoid unnecessary dashboard refreshing to reduce the load on the network or backend. For example, if your data changes every hour, then you don\u2019t need to set the dashboard refresh rate to 30 seconds. Be careful with stacking graph data. The visualizations can be misleading, and hide important data. We recommend turning it off in most cases. You can check out Best practices for creating dashboards for more information. Cleanup \u00b6 Remove the resources and cluster kubectl delete all --all eksctl delete cluster --name amp-eks-ec2 Remove the AMP workspace aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Using AWS Distro for OpenTelemetry in EKS to ingest metrics into Amazon Managed Service for Prometheus"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#using-aws-distro-for-opentelemetry-in-eks-to-ingest-metrics-into-amazon-managed-service-for-prometheus","text":"In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus (AMP) . Then we're using Amazon Managed Service for Grafana (AMG) to visualize the metrics. We will be setting up an Amazon Elastic Kubernetes Service (EKS) cluster and Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete.","title":"Using AWS Distro for OpenTelemetry in EKS to ingest metrics into Amazon Managed Service for Prometheus"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#infrastructure","text":"In the following section we will be setting up the infrastructure for this recipe.","title":"Infrastructure"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#architecture","text":"The ADOT-AMP pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and send the scraped metrics to AMP. The ADOT Collector includes two AWS OpenTelemetry Collector components specific to Prometheus \u2014 the Prometheus Receiver and the AWS Prometheus Remote Write Exporter. Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP","title":"Architecture"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#prerequisites","text":"The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have docker installed into your environment.","title":"Prerequisites"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#setup-an-eks-cluster","text":"Our demo application in this recipe will be running on top of EKS. You can either use an existing EKS cluster or create one using cluster_config.yaml . This template will create a new cluster with two EC2 t2.large nodes. Edit the template file and set your region to one of the available regions for AMP: us-east-1 us-east-2 us-west-2 eu-central-1 eu-west-1 Make sure to overwrite this region in your session, for example in bash: export AWS_DEFAULT_REGION=eu-west-1 Create your cluster using the following command. eksctl create cluster -f cluster-config.yaml","title":"Setup an EKS cluster"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#setup-an-ecr-repository","text":"In order to deploy our application to EKS we need a container registry. You can use the following command to create a new ECR registry in your account. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region eu-west-1","title":"Setup an ECR repository"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#setup-amp","text":"create a workspace using the AWS CLI aws amp create-workspace --alias prometheus-sample-app Verify the workspace is created using: aws amp list-workspaces Info For more details check out the AMP Getting started guide.","title":"Setup AMP"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#setup-adot-collector","text":"Download the template file prometheus-deamonset.yaml and edit this file with the parameters described in the next steps. In this example, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. Use the following steps to edit the downloaded file for your environment: 1. Replace <REGION> with your current Region. 2. Replace <YOUR_ENDPOINT> with your AMP workspace endpoint URL. Get your AMP endpoint url by executing the following query: aws amp describe-workspace \\ --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` \\ --query 'workspace.prometheusEndpoint' 3. Finally replace your <YOUR_ACCOUNT_ID> with your current account ID. The following command will return the account ID for the current session: aws sts get-caller-identity --query Account --output text After creating deployment file we can now apply this to our cluster by using the following command: kubectl apply -f prometheus-deamonset.yaml Info For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup","title":"Setup ADOT Collector"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#setup-amg","text":"Setup a new AMG workspace using the Amazon Managed Service for Grafana \u2013 Getting Started guide. Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation.","title":"Setup AMG"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#application","text":"In this recipe we will be using a sample application from the AWS Observability repository. This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint.","title":"Application"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#build","text":"Clone the following Git repository git clone git@github.com:aws-observability/aws-otel-community.git Build the container cd ./aws-otel-community/sample-apps/prometheus docker build . -t prometheus-sample-app:latest Note If go mod fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile. Change the following line in the Docker file: RUN GO111MODULE=on go mod download to: RUN GOPROXY=direct GO111MODULE=on go mod download","title":"Build"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#push","text":"Change the region variable to the region you selected in the beginning of this guide: export REGION=\"eu-west-1\" export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` Authenticate to your default registry: aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com\" Push container to the ECR repository docker tag prometheus-sample-app:latest \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\" docker push \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\"","title":"Push"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#deploy","text":"Edit prometheus-sample-app.yaml to contain your ECR image path. Edit the deployment to reflect your image path. (example below) apiVersion: apps/v1 kind: Deployment ... spec: containers: - name: prometheus-sample-app image: \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\" # change to your image command: [\"/bin/main\", \"-listen_address=0.0.0.0:8080\", \"-metric_count=10\"] ports: - name: web containerPort: 8080 ... Deploy the sample app to your cluster: kubectl apply -f prometheus-sample-app.yaml","title":"Deploy"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#end-to-end","text":"Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG.","title":"End-to-end"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#verify-your-pipeline-is-working","text":"Enter the following command to and note down the name of the collector pod: kubectl -n adot-col get pods You should be able to see a adot-collector pod in the running state: NAME READY STATUS RESTARTS AGE adot-collector-5f7448f6f6-cj7j8 1/1 Running 0 1h Our example template is already integrated with the logging exporter. Enter the following command: kubectl -n adot-col logs adot-collector From of the scraped metrics from the sample app will look like the following example: Resource labels: -> service.name: STRING(kubernetes-service-endpoints) -> host.name: STRING(192.168.16.238) -> port: STRING(8080) -> scheme: STRING(http) InstrumentationLibraryMetrics #0 Metric #0 Descriptor: -> Name: test_gauge0 -> Description: This is my gauge -> Unit: -> DataType: DoubleGauge DoubleDataPoints #0 StartTime: 0 Timestamp: 1606511460471000000 Value: 0.000000 To test whether AMP received the metrics, use awscurl . This tool enables you to send HTTP requests through the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command, and replace $AMP_ENDPOINT with the endpoint for your AMP workspace. awscurl --service=\"aps\" \\ --region=\"$REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" \\ {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}}","title":"Verify your pipeline is working"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#create-a-grafana-dashboard-in-amg","text":"You can now create a dashboard in Grafana to visualise the data you collected in the test App. Check out the AMG User Guide: Dashboards , to learn more about Grafana dashboards. When creating a new dashboard, make sure it has a meaningful name. If you are creating a dashboard to play or experiment, then put the word TEST or TMP in the name. Consider including your name or initials in the dashboard name or as a tag so that people know who owns the dashboard. Remove temporary experiment dashboards when you are done with them. If you create many related dashboards, think about how to cross-reference them for easy navigation. Refer to Best practices for managing dashboards for more information. Avoid unnecessary dashboard refreshing to reduce the load on the network or backend. For example, if your data changes every hour, then you don\u2019t need to set the dashboard refresh rate to 30 seconds. Be careful with stacking graph data. The visualizations can be misleading, and hide important data. We recommend turning it off in most cases. You can check out Best practices for creating dashboards for more information.","title":"Create a Grafana dashboard in AMG"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#cleanup","text":"Remove the resources and cluster kubectl delete all --all eksctl delete cluster --name amp-eks-ec2 Remove the AMP workspace aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Cleanup"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/","text":"Using AWS Distro for OpenTelemetry in EKS to ingest metrics into Amazon Managed Service for Prometheus \u00b6 In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus (AMP) . Then we're using Amazon Managed Service for Grafana (AMG) to visualize the metrics. We will be setting up an Amazon Elastic Kubernetes Service (EKS) cluster and Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete. Infrastructure \u00b6 In the following section we will be setting up the infrastructure for this recipe. Architecture \u00b6 The ADOT-AMP pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and send the scraped metrics to AMP. The ADOT Collector includes two AWS OpenTelemetry Collector components specific to Prometheus \u2014 the Prometheus Receiver and the AWS Prometheus Remote Write Exporter. Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have docker installed into your environment. Setup an EKS cluster \u00b6 Our demo application in this recipe will be running on top of EKS. You can either use an existing EKS cluster or create one using cluster_config.yaml . This template will create a new cluster with EKS on AWS Fargate . Edit the template file and set your region to one of the available regions for AMP: us-east-1 us-east-2 us-west-2 eu-central-1 eu-west-1 Make sure to overwrite this region in your session, for example in bash: export AWS_DEFAULT_REGION=eu-west-1 Create your cluster using the following command. eksctl create cluster -f cluster-config.yaml Setup an ECR repository \u00b6 In order to deploy our application to EKS we need a container registry. You can use the following command to create a new ECR registry in your account. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region eu-west-1 Setup AMP \u00b6 create a workspace using the AWS CLI aws amp create-workspace --alias prometheus-sample-app Verify the workspace is created using: aws amp list-workspaces Info For more details check out the AMP Getting started guide. Setup ADOT Collector \u00b6 Download the template file prometheus-fargate.yaml and edit this file with the parameters described in the next steps. In this example, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. Use the following steps to edit the downloaded file for your environment: 1. Replace <REGION> with your current Region. 2. Replace <YOUR_ENDPOINT> with your AMP workspace endpoint URL. Get your AMP endpoint url by executing the following query: aws amp describe-workspace \\ --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` \\ --query 'workspace.prometheusEndpoint' 3. Finally replace your <YOUR_ACCOUNT_ID> with your current account ID. The following command will return the account ID for the current session: aws sts get-caller-identity --query Account --output text After creating deployment file we can now apply this to our cluster by using the following command: kubectl apply -f prometheus-fargate.yaml Info For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup Setup AMG \u00b6 Setup a new AMG workspace using the Amazon Managed Service for Grafana \u2013 Getting Started guide. Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation. Application \u00b6 In this recipe we will be using a sample application from the AWS Observability repository. This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint. Build \u00b6 Clone the following Git repository git clone git@github.com:aws-observability/aws-otel-community.git Build the container cd ./aws-otel-community/sample-apps/prometheus docker build . -t prometheus-sample-app:latest Note If go mod fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile. Change the following line in the Docker file: RUN GO111MODULE=on go mod download to: RUN GOPROXY=direct GO111MODULE=on go mod download Push \u00b6 Change the region variable to the region you selected in the beginning of this guide: export REGION=\"eu-west-1\" export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` Authenticate to your default registry: aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com\" Push container to the ECR repository docker tag prometheus-sample-app:latest \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\" docker push \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\" Deploy \u00b6 Edit prometheus-sample-app.yaml to contain your ECR image path. -> show line here. Deploy the sample app to your cluster: kubectl apply -f prometheus-sample-app.yaml End-to-end \u00b6 Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG. Verify your pipeline is working \u00b6 Enter the following command to and note down the name of the collector pod: kubectl -n adot-col get pods You should be able to see a adot-collector pod in the running state: NAME READY STATUS RESTARTS AGE adot-collector-5f7448f6f6-cj7j8 1/1 Running 0 1h Our example template is already integrated with the logging exporter. Enter the following command: kubectl -n adot-col logs adot-collector From of the scraped metrics from the sample app will look like the following example: Resource labels: -> service.name: STRING(kubernetes-service-endpoints) -> host.name: STRING(192.168.16.238) -> port: STRING(8080) -> scheme: STRING(http) InstrumentationLibraryMetrics #0 Metric #0 Descriptor: -> Name: test_gauge0 -> Description: This is my gauge -> Unit: -> DataType: DoubleGauge DoubleDataPoints #0 StartTime: 0 Timestamp: 1606511460471000000 Value: 0.000000 To test whether AMP received the metrics, use awscurl . This tool enables you to send HTTP requests through the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command, and replace $AMP_ENDPOINT with the endpoint for your AMP workspace. awscurl --service=\"aps\" \\ --region=\"$REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" \\ {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}} Create a Grafana dashboard in AMG \u00b6 Use the following guides to create your first dashboard: User Guide: Dashboards Best practices for creating dashboards -> Put more content here. Cleanup \u00b6 Remove the resources and cluster kubectl delete all --all eksctl delete cluster --name amp-eks-fargate Remove the AMP workspace aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Using AWS Distro for OpenTelemetry in EKS to ingest metrics into Amazon Managed Service for Prometheus"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#using-aws-distro-for-opentelemetry-in-eks-to-ingest-metrics-into-amazon-managed-service-for-prometheus","text":"In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus (AMP) . Then we're using Amazon Managed Service for Grafana (AMG) to visualize the metrics. We will be setting up an Amazon Elastic Kubernetes Service (EKS) cluster and Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete.","title":"Using AWS Distro for OpenTelemetry in EKS to ingest metrics into Amazon Managed Service for Prometheus"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#infrastructure","text":"In the following section we will be setting up the infrastructure for this recipe.","title":"Infrastructure"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#architecture","text":"The ADOT-AMP pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and send the scraped metrics to AMP. The ADOT Collector includes two AWS OpenTelemetry Collector components specific to Prometheus \u2014 the Prometheus Receiver and the AWS Prometheus Remote Write Exporter. Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP","title":"Architecture"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#prerequisites","text":"The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have docker installed into your environment.","title":"Prerequisites"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#setup-an-eks-cluster","text":"Our demo application in this recipe will be running on top of EKS. You can either use an existing EKS cluster or create one using cluster_config.yaml . This template will create a new cluster with EKS on AWS Fargate . Edit the template file and set your region to one of the available regions for AMP: us-east-1 us-east-2 us-west-2 eu-central-1 eu-west-1 Make sure to overwrite this region in your session, for example in bash: export AWS_DEFAULT_REGION=eu-west-1 Create your cluster using the following command. eksctl create cluster -f cluster-config.yaml","title":"Setup an EKS cluster"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#setup-an-ecr-repository","text":"In order to deploy our application to EKS we need a container registry. You can use the following command to create a new ECR registry in your account. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region eu-west-1","title":"Setup an ECR repository"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#setup-amp","text":"create a workspace using the AWS CLI aws amp create-workspace --alias prometheus-sample-app Verify the workspace is created using: aws amp list-workspaces Info For more details check out the AMP Getting started guide.","title":"Setup AMP"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#setup-adot-collector","text":"Download the template file prometheus-fargate.yaml and edit this file with the parameters described in the next steps. In this example, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. Use the following steps to edit the downloaded file for your environment: 1. Replace <REGION> with your current Region. 2. Replace <YOUR_ENDPOINT> with your AMP workspace endpoint URL. Get your AMP endpoint url by executing the following query: aws amp describe-workspace \\ --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` \\ --query 'workspace.prometheusEndpoint' 3. Finally replace your <YOUR_ACCOUNT_ID> with your current account ID. The following command will return the account ID for the current session: aws sts get-caller-identity --query Account --output text After creating deployment file we can now apply this to our cluster by using the following command: kubectl apply -f prometheus-fargate.yaml Info For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup","title":"Setup ADOT Collector"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#setup-amg","text":"Setup a new AMG workspace using the Amazon Managed Service for Grafana \u2013 Getting Started guide. Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation.","title":"Setup AMG"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#application","text":"In this recipe we will be using a sample application from the AWS Observability repository. This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint.","title":"Application"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#build","text":"Clone the following Git repository git clone git@github.com:aws-observability/aws-otel-community.git Build the container cd ./aws-otel-community/sample-apps/prometheus docker build . -t prometheus-sample-app:latest Note If go mod fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile. Change the following line in the Docker file: RUN GO111MODULE=on go mod download to: RUN GOPROXY=direct GO111MODULE=on go mod download","title":"Build"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#push","text":"Change the region variable to the region you selected in the beginning of this guide: export REGION=\"eu-west-1\" export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` Authenticate to your default registry: aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com\" Push container to the ECR repository docker tag prometheus-sample-app:latest \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\" docker push \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/prometheus-sample-app:latest\"","title":"Push"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#deploy","text":"Edit prometheus-sample-app.yaml to contain your ECR image path. -> show line here. Deploy the sample app to your cluster: kubectl apply -f prometheus-sample-app.yaml","title":"Deploy"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#end-to-end","text":"Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG.","title":"End-to-end"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#verify-your-pipeline-is-working","text":"Enter the following command to and note down the name of the collector pod: kubectl -n adot-col get pods You should be able to see a adot-collector pod in the running state: NAME READY STATUS RESTARTS AGE adot-collector-5f7448f6f6-cj7j8 1/1 Running 0 1h Our example template is already integrated with the logging exporter. Enter the following command: kubectl -n adot-col logs adot-collector From of the scraped metrics from the sample app will look like the following example: Resource labels: -> service.name: STRING(kubernetes-service-endpoints) -> host.name: STRING(192.168.16.238) -> port: STRING(8080) -> scheme: STRING(http) InstrumentationLibraryMetrics #0 Metric #0 Descriptor: -> Name: test_gauge0 -> Description: This is my gauge -> Unit: -> DataType: DoubleGauge DoubleDataPoints #0 StartTime: 0 Timestamp: 1606511460471000000 Value: 0.000000 To test whether AMP received the metrics, use awscurl . This tool enables you to send HTTP requests through the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command, and replace $AMP_ENDPOINT with the endpoint for your AMP workspace. awscurl --service=\"aps\" \\ --region=\"$REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" \\ {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}}","title":"Verify your pipeline is working"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#create-a-grafana-dashboard-in-amg","text":"Use the following guides to create your first dashboard: User Guide: Dashboards Best practices for creating dashboards -> Put more content here.","title":"Create a Grafana dashboard in AMG"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#cleanup","text":"Remove the resources and cluster kubectl delete all --all eksctl delete cluster --name amp-eks-fargate Remove the AMP workspace aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Cleanup"},{"location":"recipes/servicemesh-monitoring-ampamg/","text":"Using Amazon Managed Service for Prometheus to monitor App Mesh environment configured on EKS \u00b6 In this recipe we show you how to ingest App Mesh envoy metrics in an Amazon EKS cluster to Amazon Managed Service for Prometheus AMP and create a custom dashboard on Amazon Managed Service for Grafana to monitor the health and performance of microservices. As part of the implementation, we will create an AMP workspace, install the App Mesh Controller for Kubernetes and inject the envoy container into the pods. We will be collecting the envoy metrics using Grafana Agent configured in the EKS Cluster Amazon Elastic Kubernetes Service (EKS) cluster and write them to AMP. Finally, we will be creating an AMG workspace and configure the AMP as the datasource and create a custom dashboard. Note This guide will take approximately 45 minutes to complete. Infrastructure \u00b6 In the following section we will be setting up the infrastructure for this recipe. Architecture \u00b6 The Grafana agent is configured to scrape the envoy metrics and ingest them to AMP through the AMP remote write endpoint Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have docker installed into your environment. You need AMP workspace configured in your AWS account Create-workspace You need to install helm You need to enable AWS-SSO Setup an EKS cluster \u00b6 First, create an EKS cluster that will be enabled with App Mesh for running the sample application. The eksctl CLI tool will be used to deploy the cluster using the eks-cluster-config.yaml . This template will create a new cluster with EKS. Edit the template file and set your region to one of the available regions for AMP: us-east-1 us-east-2 us-west-2 eu-central-1 eu-west-1 Make sure to overwrite this region in your session, for example in bash: export AWS_REGION=eu-west-1 Create your cluster using the following command. eksctl create cluster -f eks-cluster-config.yaml This creates an EKS cluster named AMP-EKS-CLUSTER and a service account named appmesh-controller that will be used by the App Mesh controller for EKS. Install the App Mesh Controller \u00b6 Next, we will run the below commands to install the App Mesh Controller and configure the Custom Resource Definitions helm repo add eks https://aws.github.io/eks-charts helm upgrade -i appmesh-controller eks/appmesh-controller \\ --namespace appmesh-system \\ --set region=${AWS_REGION} \\ --set serviceAccount.create=false \\ --set serviceAccount.name=appmesh-controller Setup AMP \u00b6 The AMP workspace is used to ingest the Prometheus metrics collected from envoy. A workspace is a logical and isolated Prometheus server dedicated to Prometheus resources such as metrics. A workspace supports fine-grained access control for authorizing its management such as update, list, describe, and delete, and the ingestion and querying of metrics. create a workspace using the AWS CLI aws amp create-workspace --alias AMP-APPMESH --region $AWS_REGION Add the necessary helm repositories helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add kube-state-metrics https://kubernetes.github.io/kube-state-metrics Info For more details check out the AMP Getting started guide. Scrape the metrics \u00b6 AMP does not directly scrape operational metrics from containerized workloads in a Kubernetes cluster. You must deploy and manage a Prometheus server or an OpenTelemetry agent such as the AWS Distro for OpenTelemetry Collector or the Grafana Agent to perform this task. In this receipe, we walk you through the process of configuring the Grafana Agent to scrape the envoy metrics and analyze them using AMP and AMG. Configure the Grafana Agent for AMP \u00b6 The Grafana Agent is a lightweight alternative to running a full Prometheus server. It keeps the necessary parts for discovering and scraping Prometheus exporters and sending metrics to a Prometheus-compatible backend, which in this case is AMP, and removes subsystems such as the storage, query, and alerting engines. The Grafana Agent is 100 percent compatible with Prometheus metrics and uses the Prometheus service discovery, scraping, write-ahead log, and remote write mechanisms from the Prometheus project. The Grafana Agent also supports basic sharding across every node in your Amazon EKS cluster by only collecting metrics running on the same node as the Grafana Agent pod, removing the need to decide between one giant machine to collect all of your Prometheus metrics and sharding through multiple manually managed Prometheus configurations. The Grafana Agent also includes native support for AWS Signature Version 4 for AWS Identity and Access Management (IAM) authentication, which means you don\u2019t need to run a sidecar Signature Version 4 proxy, which reduces complexity, memory, and CPU demand. In this receipe, we walk through the steps to configure an IAM role to send Prometheus metrics to AMP. We install the Grafana Agent on the EKS cluster and forward metrics to AMP. Configure Permissions \u00b6 The Grafana Agent scrapes operational metrics from containerized workloads running in the Amazon EKS cluster and sends them to AMP for long-term storage and subsequent querying by monitoring tools such as Grafana. Data sent to AMP must be signed with valid AWS credentials using the AWS Signature Version 4 algorithm to authenticate and authorize each client request for the managed service. The Grafana Agent can be deployed to an Amazon EKS cluster to run under the identity of a Kubernetes service account. With IAM roles for service accounts (IRSA), you can associate an IAM role with a Kubernetes service account and thus provide IAM permissions to any pod that uses the service account. This follows the principle of least privilege by using IRSA to securely configure the Grafana Agent, which includes the AWS Signature Version 4 that helps ingest Prometheus metrics to AMP. The agent-permissions-aks shell script can be used to execute the following actions. Replace the placeholder variable YOUR_EKS_CLUSTER_NAME with the name of your Amazon EKS cluster. Creates an IAM role named EKS-GrafanaAgent-AMP-ServiceAccount-Role with an IAM policy that has permissions to remote-write into an AMP workspace. Creates a Kubernetes service account named grafana-agent under the grafana-agent namespace that is associated with the IAM role. Creates a trust relationship between the IAM role and the OIDC provider hosted in your Amazon EKS cluster. You need kubectl and eksctl CLI tools to run the script. They must be configured to access your Amazon EKS cluster. kubectl create namespace grafana-agent export WORKSPACE=$(aws amp list-workspaces | jq -r '.workspaces[] | select(.alias==\"AMP-APPMESH\").workspaceId') export ROLE_ARN=$(aws iam get-role --role-name EKS-GrafanaAgent-AMP-ServiceAccount-Role --query Role.Arn --output text) export NAMESPACE=\"grafana-agent\" export REMOTE_WRITE_URL=\"https://aps-workspaces.$AWS_REGION.amazonaws.com/workspaces/$WORKSPACE/api/v1/remote_write\" Now create a manifest file, grafana-agent.yaml , with the scrape configuration to extract envoy metrics and deploy the Grafana Agent. This example deploys a DaemonSet named grafana-agent and a deployment (with one replica) named grafana-agent-deployment. The grafana-agent DaemonSet collects metrics from pods on the cluster. The grafana-agent-deployment collects metrics from services that do not live on the cluster, such as the Amazon EKS control plane. At the time of this writing, this solution will not work for the AWS Fargate data plane due to the lack of support for DaemonSet. kubectl apply -f grafana-agent.yaml After the grafana-agent is deployed, it will collect the metrics and ingest them into the specified AMP workspace. Now deploy a sample application on the Amazon EKS cluster and start analyzing the metrics. Deploy a sample application on the EKS cluster \u00b6 To install an application and inject an envoy container, use the AppMesh controller for Kubernetes you created earlier. AWS App Mesh Controller for K8s manages App Mesh resources in your Kubernetes clusters. The controller is accompanied by CRDs that allow you to define App Mesh components, such as meshes and virtual nodes, using the Kubernetes API just as you define native Kubernetes objects, such as deployments and services. These custom resources map to App Mesh API objects that the controller manages for you. The controller watches these custom resources for changes and reflects those changes into the App Mesh API. ## Install the base application git clone https://github.com/aws/aws-app-mesh-examples.git kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/1_base_application kubectl get all -n prod ## check the pod status and make sure it is running NAME READY STATUS RESTARTS AGE pod/dj-cb77484d7-gx9vk 1/1 Running 0 6m8s pod/jazz-v1-6b6b6dd4fc-xxj9s 1/1 Running 0 6m8s pod/metal-v1-584b9ccd88-kj7kf 1/1 Running 0 6m8s ## Now install the App Mesh controller and meshify the deployment kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/2_meshed_application/ kubectl rollout restart deployment -n prod dj jazz-v1 metal-v1 kubectl get all -n prod ## Now we should see two containers running in each pod NAME READY STATUS RESTARTS AGE dj-7948b69dff-z6djf 2/2 Running 0 57s jazz-v1-7cdc4fc4fc-wzc5d 2/2 Running 0 57s metal-v1-7f499bb988-qtx7k 2/2 Running 0 57s ## generate the traffic for 5 mins and we will visualize it in grafana dj_pod=`kubectl get pod -n prod --no-headers -l app=dj -o jsonpath='{.items[*].metadata.name}'` loop_counter=0 while [ $loop_counter -le 300 ] ; do kubectl exec -n prod -it $dj_pod -c dj -- curl jazz.prod.svc.cluster.local:9080 ; echo ; loop_counter=$[$loop_counter+1] ; done Create an AMG workspace \u00b6 Creating an AMG workspace is straightforward. Follow the steps in the Getting Started with Amazon Managed Service for Grafana blog post . To grant users access to the dashboard, you must enable AWS SSO. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group. By default, the user has a user type of viewer. Change the user type based on the user role. Add the AMP workspace as the data source and then start creating the dashboard. In this example, the user name is grafana-admin and the user type is Admin. Select the required data source. Review the configuration, and then choose Create workspace. Configure the data source and custom dashboard \u00b6 To configure AMP as a data source, in the Data sources section, choose Configure in Grafana, which will launch a Grafana workspace in the browser. You can also manually launch the Grafana workspace URL in the browser. Use the instructions in the Getting Started with Amazon Managed Service for Grafana blog post and specify the AMP workspace you created earlier as a data source. As you can see from the screenshots, you can view envoy metrics like downstream latency, connections, response code, and more. You can use the filters shown to drill down to the envoy metrics of a particular application. After the data source is configured, import a custom dashboard to analyze the envoy metrics. Choose Import (shown below), and then enter the ID 11022. This will import the Envoy Global dashboard so you can start analyzing the envoy metrics. End-to-End \u00b6 Configure alerts on AMG \u00b6 You can configure Grafana alerts when the metric increases beyond the intended threshold. With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification. Currently, AMG supports Amazon SNS, Opsgenie, Slack, PagerDuty, VictorOp notifier types. Before you create alert rules, you must create a notification channel. In this example, configure Amazon SNS as a notification channel. The SNS topic must be prefixed with grafana for notifications to be successfully published to the topic. Use the following command to create an SNS topic named grafana-notification and subscribe an email address. Make sure you specify the region and account id in the below command: aws sns create-topic --name grafana-notification aws sns subscribe --topic-arn arn:aws:sns:<region>:<account-id>:grafana-notification --protocol email --notification-endpoint <email-id> Add a new notification channel from the Grafana dashboard. Configure the new notification channel named grafana-notification. For Type, use AWS SNS from the drop down. For Topic, use the ARN of the SNS topic you just created. For Auth provider, choose AWS SDK Default. Now configure an alert if downstream latency exceeds five milliseconds in a one-minute period. In the dashboard, choose Downstream latency from the dropdown, and then choose Edit. On the Alert tab of the graph panel, configure how often the alert rule should be evaluated and the conditions that must be met for the alert to change state and initiate its notifications. In the following configuration, an alert is created if the downstream latency exceeds the threshold and notification will be sent through the configured grafana-alert-notification channel to the SNS topic. Cleanup \u00b6 Remove the resources and cluster kubectl delete all --all eksctl delete cluster --name AMP-EKS-CLUSTER Remove the AMP workspace aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Using Amazon Managed Service for Prometheus to monitor App Mesh environment configured on EKS"},{"location":"recipes/servicemesh-monitoring-ampamg/#using-amazon-managed-service-for-prometheus-to-monitor-app-mesh-environment-configured-on-eks","text":"In this recipe we show you how to ingest App Mesh envoy metrics in an Amazon EKS cluster to Amazon Managed Service for Prometheus AMP and create a custom dashboard on Amazon Managed Service for Grafana to monitor the health and performance of microservices. As part of the implementation, we will create an AMP workspace, install the App Mesh Controller for Kubernetes and inject the envoy container into the pods. We will be collecting the envoy metrics using Grafana Agent configured in the EKS Cluster Amazon Elastic Kubernetes Service (EKS) cluster and write them to AMP. Finally, we will be creating an AMG workspace and configure the AMP as the datasource and create a custom dashboard. Note This guide will take approximately 45 minutes to complete.","title":"Using Amazon Managed Service for Prometheus to monitor App Mesh environment configured on EKS"},{"location":"recipes/servicemesh-monitoring-ampamg/#infrastructure","text":"In the following section we will be setting up the infrastructure for this recipe.","title":"Infrastructure"},{"location":"recipes/servicemesh-monitoring-ampamg/#architecture","text":"The Grafana agent is configured to scrape the envoy metrics and ingest them to AMP through the AMP remote write endpoint Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP","title":"Architecture"},{"location":"recipes/servicemesh-monitoring-ampamg/#prerequisites","text":"The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have docker installed into your environment. You need AMP workspace configured in your AWS account Create-workspace You need to install helm You need to enable AWS-SSO","title":"Prerequisites"},{"location":"recipes/servicemesh-monitoring-ampamg/#setup-an-eks-cluster","text":"First, create an EKS cluster that will be enabled with App Mesh for running the sample application. The eksctl CLI tool will be used to deploy the cluster using the eks-cluster-config.yaml . This template will create a new cluster with EKS. Edit the template file and set your region to one of the available regions for AMP: us-east-1 us-east-2 us-west-2 eu-central-1 eu-west-1 Make sure to overwrite this region in your session, for example in bash: export AWS_REGION=eu-west-1 Create your cluster using the following command. eksctl create cluster -f eks-cluster-config.yaml This creates an EKS cluster named AMP-EKS-CLUSTER and a service account named appmesh-controller that will be used by the App Mesh controller for EKS.","title":"Setup an EKS cluster"},{"location":"recipes/servicemesh-monitoring-ampamg/#install-the-app-mesh-controller","text":"Next, we will run the below commands to install the App Mesh Controller and configure the Custom Resource Definitions helm repo add eks https://aws.github.io/eks-charts helm upgrade -i appmesh-controller eks/appmesh-controller \\ --namespace appmesh-system \\ --set region=${AWS_REGION} \\ --set serviceAccount.create=false \\ --set serviceAccount.name=appmesh-controller","title":"Install the App Mesh Controller"},{"location":"recipes/servicemesh-monitoring-ampamg/#setup-amp","text":"The AMP workspace is used to ingest the Prometheus metrics collected from envoy. A workspace is a logical and isolated Prometheus server dedicated to Prometheus resources such as metrics. A workspace supports fine-grained access control for authorizing its management such as update, list, describe, and delete, and the ingestion and querying of metrics. create a workspace using the AWS CLI aws amp create-workspace --alias AMP-APPMESH --region $AWS_REGION Add the necessary helm repositories helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add kube-state-metrics https://kubernetes.github.io/kube-state-metrics Info For more details check out the AMP Getting started guide.","title":"Setup AMP"},{"location":"recipes/servicemesh-monitoring-ampamg/#scrape-the-metrics","text":"AMP does not directly scrape operational metrics from containerized workloads in a Kubernetes cluster. You must deploy and manage a Prometheus server or an OpenTelemetry agent such as the AWS Distro for OpenTelemetry Collector or the Grafana Agent to perform this task. In this receipe, we walk you through the process of configuring the Grafana Agent to scrape the envoy metrics and analyze them using AMP and AMG.","title":"Scrape the metrics"},{"location":"recipes/servicemesh-monitoring-ampamg/#configure-the-grafana-agent-for-amp","text":"The Grafana Agent is a lightweight alternative to running a full Prometheus server. It keeps the necessary parts for discovering and scraping Prometheus exporters and sending metrics to a Prometheus-compatible backend, which in this case is AMP, and removes subsystems such as the storage, query, and alerting engines. The Grafana Agent is 100 percent compatible with Prometheus metrics and uses the Prometheus service discovery, scraping, write-ahead log, and remote write mechanisms from the Prometheus project. The Grafana Agent also supports basic sharding across every node in your Amazon EKS cluster by only collecting metrics running on the same node as the Grafana Agent pod, removing the need to decide between one giant machine to collect all of your Prometheus metrics and sharding through multiple manually managed Prometheus configurations. The Grafana Agent also includes native support for AWS Signature Version 4 for AWS Identity and Access Management (IAM) authentication, which means you don\u2019t need to run a sidecar Signature Version 4 proxy, which reduces complexity, memory, and CPU demand. In this receipe, we walk through the steps to configure an IAM role to send Prometheus metrics to AMP. We install the Grafana Agent on the EKS cluster and forward metrics to AMP.","title":"Configure the Grafana Agent for AMP"},{"location":"recipes/servicemesh-monitoring-ampamg/#configure-permissions","text":"The Grafana Agent scrapes operational metrics from containerized workloads running in the Amazon EKS cluster and sends them to AMP for long-term storage and subsequent querying by monitoring tools such as Grafana. Data sent to AMP must be signed with valid AWS credentials using the AWS Signature Version 4 algorithm to authenticate and authorize each client request for the managed service. The Grafana Agent can be deployed to an Amazon EKS cluster to run under the identity of a Kubernetes service account. With IAM roles for service accounts (IRSA), you can associate an IAM role with a Kubernetes service account and thus provide IAM permissions to any pod that uses the service account. This follows the principle of least privilege by using IRSA to securely configure the Grafana Agent, which includes the AWS Signature Version 4 that helps ingest Prometheus metrics to AMP. The agent-permissions-aks shell script can be used to execute the following actions. Replace the placeholder variable YOUR_EKS_CLUSTER_NAME with the name of your Amazon EKS cluster. Creates an IAM role named EKS-GrafanaAgent-AMP-ServiceAccount-Role with an IAM policy that has permissions to remote-write into an AMP workspace. Creates a Kubernetes service account named grafana-agent under the grafana-agent namespace that is associated with the IAM role. Creates a trust relationship between the IAM role and the OIDC provider hosted in your Amazon EKS cluster. You need kubectl and eksctl CLI tools to run the script. They must be configured to access your Amazon EKS cluster. kubectl create namespace grafana-agent export WORKSPACE=$(aws amp list-workspaces | jq -r '.workspaces[] | select(.alias==\"AMP-APPMESH\").workspaceId') export ROLE_ARN=$(aws iam get-role --role-name EKS-GrafanaAgent-AMP-ServiceAccount-Role --query Role.Arn --output text) export NAMESPACE=\"grafana-agent\" export REMOTE_WRITE_URL=\"https://aps-workspaces.$AWS_REGION.amazonaws.com/workspaces/$WORKSPACE/api/v1/remote_write\" Now create a manifest file, grafana-agent.yaml , with the scrape configuration to extract envoy metrics and deploy the Grafana Agent. This example deploys a DaemonSet named grafana-agent and a deployment (with one replica) named grafana-agent-deployment. The grafana-agent DaemonSet collects metrics from pods on the cluster. The grafana-agent-deployment collects metrics from services that do not live on the cluster, such as the Amazon EKS control plane. At the time of this writing, this solution will not work for the AWS Fargate data plane due to the lack of support for DaemonSet. kubectl apply -f grafana-agent.yaml After the grafana-agent is deployed, it will collect the metrics and ingest them into the specified AMP workspace. Now deploy a sample application on the Amazon EKS cluster and start analyzing the metrics.","title":"Configure Permissions"},{"location":"recipes/servicemesh-monitoring-ampamg/#deploy-a-sample-application-on-the-eks-cluster","text":"To install an application and inject an envoy container, use the AppMesh controller for Kubernetes you created earlier. AWS App Mesh Controller for K8s manages App Mesh resources in your Kubernetes clusters. The controller is accompanied by CRDs that allow you to define App Mesh components, such as meshes and virtual nodes, using the Kubernetes API just as you define native Kubernetes objects, such as deployments and services. These custom resources map to App Mesh API objects that the controller manages for you. The controller watches these custom resources for changes and reflects those changes into the App Mesh API. ## Install the base application git clone https://github.com/aws/aws-app-mesh-examples.git kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/1_base_application kubectl get all -n prod ## check the pod status and make sure it is running NAME READY STATUS RESTARTS AGE pod/dj-cb77484d7-gx9vk 1/1 Running 0 6m8s pod/jazz-v1-6b6b6dd4fc-xxj9s 1/1 Running 0 6m8s pod/metal-v1-584b9ccd88-kj7kf 1/1 Running 0 6m8s ## Now install the App Mesh controller and meshify the deployment kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/2_meshed_application/ kubectl rollout restart deployment -n prod dj jazz-v1 metal-v1 kubectl get all -n prod ## Now we should see two containers running in each pod NAME READY STATUS RESTARTS AGE dj-7948b69dff-z6djf 2/2 Running 0 57s jazz-v1-7cdc4fc4fc-wzc5d 2/2 Running 0 57s metal-v1-7f499bb988-qtx7k 2/2 Running 0 57s ## generate the traffic for 5 mins and we will visualize it in grafana dj_pod=`kubectl get pod -n prod --no-headers -l app=dj -o jsonpath='{.items[*].metadata.name}'` loop_counter=0 while [ $loop_counter -le 300 ] ; do kubectl exec -n prod -it $dj_pod -c dj -- curl jazz.prod.svc.cluster.local:9080 ; echo ; loop_counter=$[$loop_counter+1] ; done","title":"Deploy a sample application on the EKS cluster"},{"location":"recipes/servicemesh-monitoring-ampamg/#create-an-amg-workspace","text":"Creating an AMG workspace is straightforward. Follow the steps in the Getting Started with Amazon Managed Service for Grafana blog post . To grant users access to the dashboard, you must enable AWS SSO. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group. By default, the user has a user type of viewer. Change the user type based on the user role. Add the AMP workspace as the data source and then start creating the dashboard. In this example, the user name is grafana-admin and the user type is Admin. Select the required data source. Review the configuration, and then choose Create workspace.","title":"Create an AMG workspace"},{"location":"recipes/servicemesh-monitoring-ampamg/#configure-the-data-source-and-custom-dashboard","text":"To configure AMP as a data source, in the Data sources section, choose Configure in Grafana, which will launch a Grafana workspace in the browser. You can also manually launch the Grafana workspace URL in the browser. Use the instructions in the Getting Started with Amazon Managed Service for Grafana blog post and specify the AMP workspace you created earlier as a data source. As you can see from the screenshots, you can view envoy metrics like downstream latency, connections, response code, and more. You can use the filters shown to drill down to the envoy metrics of a particular application. After the data source is configured, import a custom dashboard to analyze the envoy metrics. Choose Import (shown below), and then enter the ID 11022. This will import the Envoy Global dashboard so you can start analyzing the envoy metrics.","title":"Configure the data source and custom dashboard"},{"location":"recipes/servicemesh-monitoring-ampamg/#end-to-end","text":"","title":"End-to-End"},{"location":"recipes/servicemesh-monitoring-ampamg/#configure-alerts-on-amg","text":"You can configure Grafana alerts when the metric increases beyond the intended threshold. With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification. Currently, AMG supports Amazon SNS, Opsgenie, Slack, PagerDuty, VictorOp notifier types. Before you create alert rules, you must create a notification channel. In this example, configure Amazon SNS as a notification channel. The SNS topic must be prefixed with grafana for notifications to be successfully published to the topic. Use the following command to create an SNS topic named grafana-notification and subscribe an email address. Make sure you specify the region and account id in the below command: aws sns create-topic --name grafana-notification aws sns subscribe --topic-arn arn:aws:sns:<region>:<account-id>:grafana-notification --protocol email --notification-endpoint <email-id> Add a new notification channel from the Grafana dashboard. Configure the new notification channel named grafana-notification. For Type, use AWS SNS from the drop down. For Topic, use the ARN of the SNS topic you just created. For Auth provider, choose AWS SDK Default. Now configure an alert if downstream latency exceeds five milliseconds in a one-minute period. In the dashboard, choose Downstream latency from the dropdown, and then choose Edit. On the Alert tab of the graph panel, configure how often the alert rule should be evaluated and the conditions that must be met for the alert to change state and initiate its notifications. In the following configuration, an alert is created if the downstream latency exceeds the threshold and notification will be sent through the configured grafana-alert-notification channel to the SNS topic.","title":"Configure alerts on AMG"},{"location":"recipes/servicemesh-monitoring-ampamg/#cleanup","text":"Remove the resources and cluster kubectl delete all --all eksctl delete cluster --name AMP-EKS-CLUSTER Remove the AMP workspace aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Cleanup"}]}